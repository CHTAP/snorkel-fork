{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Create a new database in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "#set this user line \n",
    "user = 'jared_local'\n",
    "\n",
    "PARALLEL = 4 # assuming a quad-core machine\n",
    "ATTRIBUTE = \"entity_phone\"\n",
    "os.environ['SNORKELDBNAME'] = \"location_extraction\"\n",
    "\n",
    "if user == 'accenture':\n",
    "    os.environ['SNORKELDB'] = 'postgresql://localhost:5432/' + os.environ['SNORKELDBNAME']\n",
    "    sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/')\n",
    "elif user == 'jared':\n",
    "    os.environ['SNORKELDB'] = 'postgres://jdunnmon:123@localhost:5432/' + os.environ['SNORKELDBNAME']\n",
    "    sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/chtap/')\n",
    "elif user == 'jared_local':\n",
    "    os.environ['SNORKELDB'] = 'postgres://jdunnmon:genpass2014@localhost:5432/' + os.environ['SNORKELDBNAME']\n",
    "    sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/chtap/')\n",
    "    \n",
    "#from sqlalchemy import create_engine\n",
    "#snorkeldb = create_engine('postgresql://localhost:5432/', isolation_level=\"AUTOCOMMIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.1 Defining a Candidate Schema2) Candidate Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "\n",
    "Location_Extraction = candidate_subclass('location_extraction', [\"location\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parsing and Transforming the Input Documents into Unified Data Models\n",
    "\n",
    "### Configuring an `HTMLPreprocessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "\n",
    "if user == 'accenture':\n",
    "    docs_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/data/profiles_chtap/'\n",
    "elif user == 'jared':\n",
    "    docs_path = '/lfs/local/0/jdunnmon/chtap/data/s3/chtap_profiles_20170928/'\n",
    "elif user == 'jared_local':\n",
    "    docs_path = '/home/jdunnmon/research/re/projects/memex/data/profiles/crawl_october_2017/texas_profiles_data'\n",
    "\n",
    "doc_preprocessor = HTMLPreprocessor(docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring an `OmniParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 9.54 s, sys: 112 ms, total: 9.65 s\n",
      "Wall time: 16min 38s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = OmniParser(structural=True, lingual=True)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 342\n",
      "Phrases: 71150\n",
      "Table 832\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.models import Document, Phrase,Table\n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Phrases:\", session.query(Phrase).count()\n",
    "print \"Table\", session.query(Table).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dividing the Corpus into Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 274\n",
      "dev: 34\n",
      "test: 34\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "splits = (0.8, 0.9)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "from pprint import pprint\n",
    "#pprint([x.name for x in train_docs])\n",
    "print \"train:\",len(train_docs)\n",
    "print \"dev:\" ,len(dev_docs)\n",
    "print \"test:\",len(test_docs)\n",
    "# from pprint import pprint\n",
    "# pprint([x.name for x in train_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Candidate Extraction & Multimodal Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import *\n",
    "location_matcher = LocationMatcher(longest_match_only=True) \n",
    "\n",
    "####Define a relation's ContextSpaces\n",
    "\n",
    "from snorkel.contrib.fonduer.fonduer.candidates import OmniNgrams\n",
    "location_ngrams = OmniNgrams(n_max=6, split_tokens=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining candidate Throttlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.lf_helpers import *\n",
    "import re\n",
    "from snorkel.lf_helpers import *\n",
    "\n",
    "\n",
    "    \n",
    "def location_currencies_filter(location):\n",
    "    list_currencies = [ \"dollar\", \"dollars\", \"lira\",\"kwacha\",\"rials\",\"rial\",\"dong\",\"dongs\",\"fuerte\",\"euro\",\n",
    "                       \"euros\",\"vatu\",\"som\",\"peso\",\"sterling\",\"sterlings\",\"soms\",\"pestos\",\n",
    "                       \"pounds\", \n",
    "                  \"pound\",\"dirham\",\"dirhams\",\"hryvnia\",\"manat\",\"manats\",\"liras\",\"lira\",\n",
    "                       \"dinar\",\"dinars\",\"pa'anga\",\"franc\",\"baht\",\"schilling\",\n",
    "                  \"somoni\",\"krona\",\"lilangeni\",\"rupee\",\"rand\",\"shilling\",\"leone\",\"riyal\",\"dobra\",\n",
    "                  \"tala\",\"ruble\",\"zloty\",\"peso\",\"sol\",\"quarani\",\"kina\",\"guinean\",\"balboa\",\"krone\",\"naira\",\n",
    "                  \"cordoba\",\"kyat\",\"metical\",\"togrog\",\"leu\",\"ouguiya\",\"rufiyaa\",\"ringgit\",\"kwacha\",\n",
    "                  \"ariary\",\"denar\",\"litas\",\"loti\",\"lats\",\"kip\",\"som\",\"won\",\"tenge\",\"yen\",\"shekel\",\"rupiah\",\n",
    "                  \"forint\",\"lempira\",\"gourde\",\"quetzal\",\"cedi\",\"lari\",\"dalasi\",\"cfp\",\"birr\",\"kroon\",\"nakfa\",\n",
    "                  \"cfa\",\"Peso\",\"koruna\",\"croatian\",\"colon\",\"yuan\",\"escudo\",\"cape\",\"riel\",\"lev\",\"real\"\n",
    "                  ,\"real\",\"mark\",\"boliviano\",\"ngultrum\",\"taka\",\"manat\",\"dram\",\"kwanza\",\"lek\",\"afghani\",\"renminbi\"]\n",
    "\n",
    "    \n",
    "    cand_right_tokens = list(get_right_ngrams(location,window=2))\n",
    "    for cand in cand_right_tokens:\n",
    "        if cand not in list_currencies:\n",
    "            return location\n",
    "    \n",
    "candidate_filter = location_currencies_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 64 ms, sys: 32 ms, total: 96 ms\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.candidates import CandidateExtractor\n",
    "\n",
    "candidate_extractor = CandidateExtractor(Location_Extraction,\n",
    "                                         [location_ngrams], [location_matcher],\n",
    "                                         candidate_filter=candidate_filter)\n",
    "\n",
    "%time candidate_extractor.apply(train_docs, split=0, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 1932\n"
     ]
    }
   ],
   "source": [
    "train_cands = session.query(Location_Extraction).filter(Location_Extraction.split == 0).all()\n",
    "print \"Number of candidates:\", len(train_cands) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the candidate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.fonduer.lf_helpers import*\n",
    "from snorkel.contrib.fonduer.candidates import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_extraction(Span(\"Dallas\", sentence=135702, chars=[35,40], words=[11,11]))\n",
      "location_extraction(Span(\"Dallas\", sentence=190784, chars=[13,18], words=[3,3]))\n",
      "location_extraction(Span(\"United States\", sentence=151353, chars=[0,12], words=[0,1]))\n"
     ]
    }
   ],
   "source": [
    "cand_16= train_cands[16]\n",
    "print cand_16\n",
    "cand_18= train_cands[18]\n",
    "print cand_18\n",
    "cand_19= train_cands[19]\n",
    "print cand_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text for the 16th candidate:\n",
      "Phrase (Doc: a2de2646-0ee6-40b0-8631-d9188ea6d96b, Index: 74, Text: var as_sid = '16';     var ad_loc='Dallas';   var aspublisher_width = \"200\";  var aspublisher_height = \"700\";  var aspublis_color_bg = \"ffffff\";  var aspublis_color_border = \"ffffff\";  var aspublis_color_link = \"006621\";  var aspublis_color_text = \"000000\";  var aspublis_color_url = \"1a0dab\";  as_show_ad('page_ads_2', as_sid);)\n",
      "16th candidate\n",
      ": location_extraction(Span(\"Dallas\", sentence=135702, chars=[35,40], words=[11,11]))\n",
      "ancestor of 16th candidate\n",
      ": ['html', 'body', 'div', 'div', 'script']\n",
      "***************************************************\n",
      "text for the 17th candidate:\n",
      "Phrase (Doc: 0d40b595-921b-4063-84f1-01a1e44be0f4, Index: 28, Text: Dallas, Texas Female Escort & GFE available for Incall.)\n",
      "17th candidate: location_extraction(Span(\"Dallas\", sentence=119801, chars=[0,5], words=[0,0]))\n",
      "ancestor of 17th candidate\n",
      ": ['html', 'body', 'div', 'div', 'div']\n",
      "***************************************************\n",
      "text for the 19th candidate:\n",
      "Phrase (Doc: b2766805-331e-41b1-8883-65ee0e7893f3, Index: 20, Text: United States »)\n",
      "19th candidate: location_extraction(Span(\"United States\", sentence=151353, chars=[0,12], words=[0,1]))\n",
      "ancestor of 19th candidate\n",
      ": ['html', 'body', 'div', 'div', 'table', 'tr', 'td', 'a']\n"
     ]
    }
   ],
   "source": [
    "cand_16= train_cands[16]\n",
    "print \"text for the 16th candidate:\\n\", cand_16.get_parent()\n",
    "print \"16th candidate\\n:\",cand_16\n",
    "ance_16 = get_ancestor_tag_names(cand_16)\n",
    "print \"ancestor of 16th candidate\\n:\", ance_16 \n",
    "print \"***************************************************\"\n",
    "cand_17= train_cands[17]\n",
    "print \"text for the 17th candidate:\\n\", cand_17.get_parent()\n",
    "print \"17th candidate:\",cand_17\n",
    "ance_17 = get_ancestor_tag_names(cand_17)\n",
    "print \"ancestor of 17th candidate\\n:\", ance_17\n",
    "print \"***************************************************\"\n",
    "\n",
    "cand_19= train_cands[19]\n",
    "print \"text for the 19th candidate:\\n\", cand_19.get_parent()\n",
    "print \"19th candidate:\",cand_19\n",
    "ance_19 = get_ancestor_tag_names(cand_18)\n",
    "print \"ancestor of 19th candidate\\n:\", ance_19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Repeating for development and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 229\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 266\n",
      "CPU times: user 5.77 s, sys: 104 ms, total: 5.87 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1)\n",
    "    print \"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 1932\n",
      "['html', 'head', 'title']\n",
      "Phrase (Doc: e89be8b6-b029-4edd-a61b-f9cc806c1b38, Index: 0, Text: 737-204-4588 Kendra James  Austin, Texas Female Escorts)\n"
     ]
    }
   ],
   "source": [
    "dev_cands = session.query(Location_Extraction).filter(Location_Extraction.split == 1).all()\n",
    "print \"Number of candidates:\", len(train_cands)\n",
    "dev_cand1= dev_cands[10]\n",
    "print get_ancestor_tag_names(dev_cand1)\n",
    "print dev_cand1.get_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span(\"United States\", sentence=153831, chars=[0,12], words=[0,1])\n",
      "Span(\"Texas\", sentence=153752, chars=[29,33], words=[4,4])\n",
      "Span(\"Houston\", sentence=191159, chars=[13,19], words=[3,3])\n",
      "Span(\"Houston\", sentence=153840, chars=[0,6], words=[0,0])\n",
      "Span(\"Houston\", sentence=153859, chars=[0,6], words=[0,0])\n",
      "Span(\"Houston\", sentence=154013, chars=[35,41], words=[11,11])\n",
      "Phrase (Doc: df84365f-614e-4689-b12d-57580ff8c059, Index: 18, Text: United States »)\n",
      "Phrase (Doc: df84365f-614e-4689-b12d-57580ff8c059, Index: 0, Text: 832-739-8609 Brenda Houston, Texas Female Escorts)\n",
      "Phrase (Doc: df84365f-614e-4689-b12d-57580ff8c059, Table: 0, Row: 0, Col: 0, Index: 0, Text: Back To All  Houston, Texas Female Escort)\n",
      "Phrase (Doc: df84365f-614e-4689-b12d-57580ff8c059, Index: 20, Text: Houston    »)\n",
      "Phrase (Doc: df84365f-614e-4689-b12d-57580ff8c059, Index: 26, Text: Houston, Texas Female Escort available for Incall.)\n",
      "Phrase (Doc: df84365f-614e-4689-b12d-57580ff8c059, Index: 64, Text: var as_sid = '16';     var ad_loc='Houston';   var aspublisher_width = \"200\";  var aspublisher_height = \"700\";  var aspublis_color_bg = \"ffffff\";  var aspublis_color_border = \"ffffff\";  var aspublis_color_link = \"006621\";  var aspublis_color_text = \"000000\";  var aspublis_color_url = \"1a0dab\";  as_show_ad('page_ads_2', as_sid);)\n"
     ]
    }
   ],
   "source": [
    "can_tmp = []\n",
    "dc_ind=50\n",
    "for can in dev_cands:\n",
    "    if can.get_parent().document.name == dev_cands[dc_ind].get_parent().document.name:\n",
    "        can_tmp.append(can.get_parent())\n",
    "        print can.location\n",
    "for cn in can_tmp:\n",
    "    print cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Place Names and Locationsfrom Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting google place and geocoding APIs\n",
    "import googlemaps as gm\n",
    "import gmaps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "maps_api_key = 'AIzaSyA0Veo5Lc6JOwDjNgQvPEhQB4AiZcrYQGI'\n",
    "gmaps.configure(api_key=maps_api_key)\n",
    "\n",
    "def get_possible_locations(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    qo: full json structure returned from API call\n",
    "    cl: list of candidate location strings\n",
    "    \"\"\" \n",
    "    api_key = 'AIzaSyDbk3lLZHuQVKDRBN99_oz-p4AJjIzhA0w'\n",
    "    gms = gm.Client(key=api_key)\n",
    "    qo = gm.places.places_autocomplete(gms,plc)\n",
    "    cl = [a['description'] for a in qo]\n",
    "    return qo,cl\n",
    "\n",
    "def get_geocode(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    qo full json structure returned from API call\n",
    "    (lat,lon): lat-lon tuple\n",
    "    \"\"\"\n",
    "    api_key = 'AIzaSyBlLyOaasYMgMxFGUh2jJyxIG0_pZFF_jM'\n",
    "    gms = gm.Client(key=api_key)\n",
    "    qo = gm.geocoding.geocode(gms,plc)\n",
    "    lat = qo[0]['geometry']['location']['lat']\n",
    "    lng = qo[0]['geometry']['location']['lng']\n",
    "    return qo,(lat,lng)\n",
    "\n",
    "def slice_pd_by_cont(dfm,col,val,pres=True,lower=False,union=False):\n",
    "    \"\"\"\n",
    "    Returns dataframe where column values include/exclude values in provided list\n",
    "    \n",
    "    INPUTS:\n",
    "    dfm: dataframe\n",
    "    col: column header\n",
    "    val: list of strings to include/ignore\n",
    "    pres: true to include, false to exclude\n",
    "    union: include union of these values\n",
    "    \"\"\"\n",
    "    if union:\n",
    "        val = ['|'.join(val)]\n",
    "    for vl in val:\n",
    "        if ~lower:\n",
    "            if pres:\n",
    "                dfm = dfm.loc[dfm[col].str.contains(vl,na=False)]\n",
    "            else:\n",
    "                dfm = dfm.loc[~dfm[col].str.contains(vl,na=False)]\n",
    "        else:\n",
    "            if pres:\n",
    "                dfm = dfm.loc[dfm[col].str.lower().str.contains(vl,na=False)]\n",
    "            else:\n",
    "                dfm = dfm.loc[~dfm[col].str.lower().str.contains(vl,na=False)]\n",
    "    return dfm\n",
    "\n",
    "def map_candidates_and_centroid(dfm):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    dfm: dataframe containing at least latitude, longitude\n",
    "    \n",
    "    OUTPUT\n",
    "    centroid: np array of lat/lon of location centroid\n",
    "    \"\"\"\n",
    "    df_cans = dfm\n",
    "    df_cans_map = dfm[['latitude','longitude']]\n",
    "    df_cans['lat_long'] = df_cans[['latitude', 'longitude']].apply(tuple, axis=1)\n",
    "    point_tup_lst = df_cans['lat_long'].tolist()\n",
    "    points = MultiPoint(point_tup_lst)\n",
    "    cent = np.array(points.centroid)\n",
    "    cent_df = pd.DataFrame([cent]) #this is a rough centroid estimate\n",
    "    fig = gmaps.Map()\n",
    "    can_layer = gmaps.symbol_layer(\n",
    "    df_cans_map, fill_color=\"green\", stroke_color=\"green\", scale=2)\n",
    "    cent_layer = gmaps.symbol_layer(\n",
    "    cent_df, fill_color=\"red\", stroke_color=\"red\", scale=2)\n",
    "    fig.add_layer(can_layer)\n",
    "    fig.add_layer(cent_layer)\n",
    "    fig\n",
    "    return cent,fig\n",
    "\n",
    "state_dict = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}\n",
    "state_add_dict = {v: k for k, v in state_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a candidate dictionary keyed by document name\n",
    "doc_dict = defaultdict(list)\n",
    "loc_dict = defaultdict(list)\n",
    "can_loc_dict = defaultdict(list)\n",
    "for can in dev_cands:\n",
    "    doc_dict[can.get_parent().document.name].append(can)\n",
    "\n",
    "    #calling API for each location\n",
    "for ky in doc_dict.keys():\n",
    "    #print doc_dict[ky]\n",
    "    for can in doc_dict[ky]:\n",
    "        loc_can = can.location.get_span()\n",
    "        can_loc_dict[ky].append(loc_can)\n",
    "    for plc in list(set(can_loc_dict[ky])):\n",
    "        _,loc_out = get_possible_locations(plc)\n",
    "        loc_dict[ky] = loc_dict[ky]+loc_out\n",
    "  #  print loc_dict[ky] \n",
    "  #  print can_loc_dict[ky]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "from collections import Counter\n",
    "    \n",
    "def get_attr(obj):\n",
    "    out = [a for a in dir(obj) if not a.startswith('__') and not callable(getattr(obj,a))]\n",
    "    return out\n",
    "\n",
    "def most_common(lt):\n",
    "    data = Counter(lt)\n",
    "    return data.most_common(1)[0][0]\n",
    "\n",
    "def get_common_country(lt):\n",
    "    country_lst = []\n",
    "    country_els = []\n",
    "    for it in lt:\n",
    "        try:\n",
    "            country = pycountry.countries.lookup(it.lower())\n",
    "            country_lst.append(country.alpha_3)\n",
    "            country_els.append(it)\n",
    "        except:\n",
    "            country = None \n",
    "    if country_lst == []:\n",
    "        return 'none',[],[]\n",
    "    return most_common(country_lst),country_lst, country_els\n",
    "\n",
    "def get_common_state(lt):\n",
    "    state_lst = []\n",
    "    state_els = []\n",
    "    for it in lt:\n",
    "        if it in state_add_dict.keys():\n",
    "            state_lst.append(it)\n",
    "            state_els.append(it)\n",
    "        elif it in state_add_dict.values():\n",
    "            state_lst.append(state_dict[it])\n",
    "            state_els.append(it)\n",
    "    if state_lst == []:\n",
    "        return 'none',[],[]\n",
    "    else:\n",
    "        return most_common(state_lst), state_lst, state_els\n",
    "\n",
    "def get_possible_locale(lt,cn,st,cn_lst,st_lst):\n",
    "    locale_list = []\n",
    "    a = [b for b in lt if b not in cn_lst and b not in st_lst]\n",
    "    for b in a:\n",
    "        locales = get_possible_locations(b)\n",
    "        locales = [c for c in locales if cn in b and st in b]\n",
    "        locale_list.append(locales)\n",
    "    return locale_list\n",
    "\n",
    "def lookup_country_name(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).name\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_state_abbrev(cn):\n",
    "    try:\n",
    "        out = state_add_dict[cn]\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "def lookup_country_alpha3(cn):\n",
    "    return pycountry.countries.lookup(cn).alpha_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Houston', u'Angel Houston', u'Houston', u'Houston', u'Houston']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "['none', 'houston', 'tx', 'united states']\n",
      "['angel lane', 'houston', 'tx', 'united states']\n",
      "['angel fire lane', 'houston', 'tx', 'united states']\n",
      "['angel shores', 'houston', 'tx', 'united states']\n",
      "['angel falls lane', 'houston', 'tx', 'united states']\n",
      "['angel island lane', 'houston', 'tx', 'united states']\n",
      "[u'Dallas', u'Dallas', u'Dallas']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'dallas', 'tx', 'united states']\n",
      "[]\n",
      "[u'none', u'none', u'TX', u'USA']\n",
      "[u'Dallas', u'Dallas', u'SEATTLE ROCK', u'Dallas', u'Anhellica Dallas', u'Dallas']\n",
      "Checking Locale 0 of 1\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'dallas', 'tx', 'united states']\n",
      "[]\n",
      "[u'none', u'none', u'TX', u'USA']\n",
      "[u'Houston', u'Houston', u'Houston', u'Houston']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'houston', 'tx', 'united states']\n",
      "[]\n",
      "[u'Austin', u'Trishia Austin', u'Austin']\n",
      "Checking Locale 0 of 2\n",
      "Checking Locale 1 of 2\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'austin', 'tx', 'united states']\n",
      "[]\n",
      "[u'none', u'none', u'TX', u'USA']\n",
      "[u'Dallas', u'Dallas', u'Honey Dallas', u'Dallas']\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "[u'Airbnb', u'Houston', u'Houston', u'Houston', u'Houston']\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'houston', 'tx', 'united states']\n",
      "[u'El Paso', u'El Paso', u'El Paso']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'el paso', 'tx', 'united states']\n",
      "[u'Austin', u'Austin']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'austin', 'tx', 'united states']\n",
      "[u'Arlington', u'Arlington', u'Nuke Town', u'Arlington', u'Arlington']\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "['nuke town sweepstakes', 'river road', 'amarillo', 'tx', 'united states']\n",
      "[u'Dallas', u'Dallas', u'Dallas']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'dallas', 'tx', 'united states']\n",
      "[u'Dallas', u'Dallas', u'Dallas', u'Dallas']\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "[u'Houston', u'Houston', u'Jasminekitt Houston', u'Houston', u'Houston']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'houston', 'tx', 'united states']\n",
      "[u'FLORIDA', u'Dallas']\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "['dallas', 'tx', 'united states']\n",
      "['dallas parkway', 'dallas', 'tx', 'united states']\n",
      "[]\n",
      "[u'none', u'none', u'TX', u'USA']\n",
      "[u'Dallas']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'dallas', 'tx', 'united states']\n",
      "[u'San Antonio', u'Sea World', u'San Antonio', u'San Antonio', u'San Antonio', u'San Antonio', u'San Antonio']\n",
      "Checking Locale 0 of 4\n",
      "Checking Locale 1 of 4\n",
      "Checking Locale 2 of 4\n",
      "Checking Locale 3 of 4\n",
      "Checking Locale 0 of 4\n",
      "Exact City Found\n",
      "['none', 'san antonio', 'tx', 'united states']\n",
      "[u'Houston', u'Houston', u'Houston', u'Houston']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'houston', 'tx', 'united states']\n",
      "[u'Dallas', u'Dallas', u'Dallas', u'Delilah Dallas']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'dallas', 'tx', 'united states']\n",
      "[u'Austin']\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "[u'Dallas', u'Dallas', u'Dallas']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'dallas', 'tx', 'united states']\n",
      "[u'Los Angeles', u'Houston Tx', u'Houston', u'Houston', u'Houston', u'Houston', u'Houston', u'Tx']\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "Checking Locale 0 of 4\n",
      "Checking Locale 1 of 4\n",
      "Checking Locale 2 of 4\n",
      "Checking Locale 3 of 4\n",
      "Checking Locale 0 of 4\n",
      "Checking Locale 1 of 4\n",
      "Checking Locale 2 of 4\n",
      "Checking Locale 3 of 4\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "['los angeles', 'ca', 'united states']\n",
      "['los angeles county', 'ca', 'united states']\n",
      "['los angeles zoo', 'los angeles', 'ca', 'united states']\n",
      "['los angeles bus station', 'east 7th street', 'los angeles', 'ca', 'united states']\n",
      "[u'Dallas', u'Dallas']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'dallas', 'tx', 'united states']\n",
      "[u'Dallas', u'Dallas', u'Dallas']\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "[u'Dallas', u'Dallas', u'Dallas', u'Dallas']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'dallas', 'tx', 'united states']\n",
      "[u'Texas', u'No Salidas', u'San Antonio', u'San Antonio', u'Dulce Latina  San Antonio', u'San Antonio', u'San Antonio']\n",
      "Checking Locale 0 of 4\n",
      "Checking Locale 1 of 4\n",
      "Checking Locale 2 of 4\n",
      "Checking Locale 3 of 4\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "[u'Gulf Freeway', u'Houston', u'Houston', u'Gulf Freeway', u'Houston', u'Houston']\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'houston', 'tx', 'united states']\n",
      "[u'Houston', u'Houston', u'Houston', u'Abbey Houston', u'Houston']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "Checking Locale 0 of 5\n",
      "Checking Locale 1 of 5\n",
      "Checking Locale 2 of 5\n",
      "Checking Locale 3 of 5\n",
      "Checking Locale 4 of 5\n",
      "['none', 'houston', 'tx', 'united states']\n",
      "['the abbey at briargrove park', 'seagler road', 'houston', 'tx', 'united states']\n",
      "['the abbey at westminster plaza', 'westminster plaza drive', 'houston', 'tx', 'united states']\n",
      "['the abbey at enclave', 'westmead drive', 'houston', 'tx', 'united states']\n",
      "['the abbey at eldridge', 'briar forest drive', 'houston', 'tx', 'united states']\n",
      "['the abbey at briar forest', 'briar forest drive', 'houston', 'tx', 'united states']\n",
      "[u'Dallas', u'Dallas', u'Dallas']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'dallas', 'tx', 'united states']\n",
      "[u'Austin', u'Austin']\n",
      "Checking Locale 0 of 5\n",
      "Exact City Found\n",
      "['none', 'austin', 'tx', 'united states']\n"
     ]
    }
   ],
   "source": [
    "#getting most common locale\n",
    "out_locales = defaultdict(list)\n",
    "for ky in can_loc_dict.keys():\n",
    "    #getting country names\n",
    "    probable_country,country_list, country_els = get_common_country(can_loc_dict[ky])\n",
    "    if pycountry.countries.lookup(probable_country).alpha_3 == 'USA' and len(can_loc_dict[ky]) >1:\n",
    "        #getting state names\n",
    "        probable_state,state_list,state_els = get_common_state(can_loc_dict[ky])\n",
    "    else:\n",
    "        probable_state,state_list,state_els = 'none',[],[]\n",
    "    \n",
    "    #getting state names\n",
    "    locale_list = []\n",
    "    a = [b for b in can_loc_dict[ky] if b not in country_els and b not in state_els] #need lookup here\n",
    "    print a\n",
    "    if a == []:\n",
    "        if probable_state != 'none' and probable_country != 'none' and a == []:\n",
    "            locale_list = ['none,none,'+state_add_dict[probable_state]+','+probable_country]\n",
    "    else:\n",
    "        most_common_locale = most_common(a)\n",
    "        aset = list(set(a))\n",
    "        #print aset\n",
    "        for b in aset:\n",
    "                #print b\n",
    "                locale_tmp = []\n",
    "                qo,locales = get_possible_locations(b)\n",
    "                not_exact = 1\n",
    "                count = 0\n",
    "                while not_exact and count<len(locales):\n",
    "                    print('Checking Locale %d of %d' %(count,len(locales)))\n",
    "                    c = locales[count]\n",
    "                    spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
    "                    if lookup_country_name(probable_country).lower() in spl:\n",
    "                        if lookup_state_abbrev(probable_state).lower() in spl: \n",
    "                            if spl[0].lower() == most_common_locale.lower() and len(spl) == 3:\n",
    "                                locale_list = ['none']+spl\n",
    "                                locale_list = [','.join(locale_list)]\n",
    "                                not_exact = 0\n",
    "                                print 'Exact City Found'\n",
    "                            elif spl[0].lower() == most_common_locale.lower() and len(spl) == 4:\n",
    "                                locale_list = [','.join(spl)]\n",
    "                                not_exact = 0\n",
    "                                print 'Exact Location Found'\n",
    "                            else:             \n",
    "                                locale_list.append(','.join(spl))  \n",
    "                                count = count+1\n",
    "                        else:\n",
    "                            count = count+1         \n",
    "                    else:\n",
    "                        count = count+1\n",
    "        \n",
    "    #reformatting for labeling comparison\n",
    "    locale_list_out = []\n",
    "    for c in locale_list:\n",
    "        b = c.split(',')\n",
    "        print b\n",
    "        b[-1] = str(lookup_country_alpha3(b[-1]).lower())\n",
    "        b[-2] = state_dict[b[-2].upper()].lower()\n",
    "        locale_list_out.append(','.join(b)) \n",
    "    out_locales[ky] = locale_list_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in out_locales.keys():\n",
    "    if out_locales[ii] == []:\n",
    "        out_locales[ii] = ['none','none','none','none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('extracted_loc_tsv.tsv','w')\n",
    "for ky in can_loc_dict.keys():\n",
    "    line = ky+\"\\t\"+out_locales[ky][0]+'\\n'\n",
    "    f.write(line)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
