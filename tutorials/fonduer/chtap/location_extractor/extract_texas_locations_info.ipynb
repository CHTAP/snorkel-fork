{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Creat a new database in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_extraction\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PARALLEL = 4 # assuming a quad-core machine\n",
    "ATTRIBUTE = \"entity_phone\"\n",
    "#os.environ['SNORKELDB']\n",
    "os.environ['SNORKELDBNAME'] = \"location_extraction\"\n",
    "print os.environ['SNORKELDBNAME']\n",
    "os.environ['SNORKELDB'] = 'postgresql://localhost:5432/' + os.environ['SNORKELDBNAME']\n",
    "#from sqlalchemy import create_engine\n",
    "#snorkeldb = create_engine('postgresql://localhost:5432/', isolation_level=\"AUTOCOMMIT\")\n",
    "\n",
    "\n",
    "sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.1 Defining a Candidate Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "\n",
    "Location_Extraction = candidate_subclass('location_extraction', [\"location\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parsing and Transforming the Input Documents into Unified Data Models\n",
    "\n",
    "### Configuring an `HTMLPreprocessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "\n",
    "docs_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/data/texas_profiles_data/'\n",
    "\n",
    "doc_preprocessor = HTMLPreprocessor(docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring an `OmniParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 8.86 s, sys: 153 ms, total: 9.01 s\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = OmniParser(structural=True, lingual=True)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 342\n",
      "Phrases: 71150\n",
      "Table 832\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.models import Document, Phrase,Table\n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Phrases:\", session.query(Phrase).count()\n",
    "print \"Table\", session.query(Table).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dividing the Corpus into Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 274\n",
      "dev: 34\n",
      "test: 34\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "splits = (0.8, 0.9)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "from pprint import pprint\n",
    "#pprint([x.name for x in train_docs])\n",
    "print \"train:\",len(train_docs)\n",
    "print \"dev:\" ,len(dev_docs)\n",
    "print \"test:\",len(test_docs)\n",
    "# from pprint import pprint\n",
    "# pprint([x.name for x in train_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Candidate Extraction & Multimodal Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import *\n",
    "location_matcher = LocationMatcher(longest_match_only=True) \n",
    "\n",
    "####Define a relation's ContextSpaces\n",
    "\n",
    "from snorkel.contrib.fonduer.fonduer.candidates import OmniNgrams\n",
    "location_ngrams = OmniNgrams(n_max=6, split_tokens=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining candidate Throttlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.lf_helpers import *\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "def location_currencies_filter(location):\n",
    "    list_currencies = [ \"dollar\", \"dollars\", \"lira\",\"kwacha\",\"rials\",\"rial\",\"dong\",\"dongs\",\"fuerte\",\"euro\",\n",
    "                       \"euros\",\"vatu\",\"som\",\"peso\",\"sterling\",\"sterlings\",\"soms\",\"pestos\",\n",
    "                       \"pounds\", \n",
    "                  \"pound\",\"dirham\",\"dirhams\",\"hryvnia\",\"manat\",\"manats\",\"liras\",\"lira\",\n",
    "                       \"dinar\",\"dinars\",\"pa'anga\",\"franc\",\"baht\",\"schilling\",\n",
    "                  \"somoni\",\"krona\",\"lilangeni\",\"rupee\",\"rand\",\"shilling\",\"leone\",\"riyal\",\"dobra\",\n",
    "                  \"tala\",\"ruble\",\"zloty\",\"peso\",\"sol\",\"quarani\",\"kina\",\"guinean\",\"balboa\",\"krone\",\"naira\",\n",
    "                  \"cordoba\",\"kyat\",\"metical\",\"togrog\",\"leu\",\"ouguiya\",\"rufiyaa\",\"ringgit\",\"kwacha\",\n",
    "                  \"ariary\",\"denar\",\"litas\",\"loti\",\"lats\",\"kip\",\"som\",\"won\",\"tenge\",\"yen\",\"shekel\",\"rupiah\",\n",
    "                  \"forint\",\"lempira\",\"gourde\",\"quetzal\",\"cedi\",\"lari\",\"dalasi\",\"cfp\",\"birr\",\"kroon\",\"nakfa\",\n",
    "                  \"cfa\",\"Peso\",\"koruna\",\"croatian\",\"colon\",\"yuan\",\"escudo\",\"cape\",\"riel\",\"lev\",\"real\"\n",
    "                  ,\"real\",\"mark\",\"boliviano\",\"ngultrum\",\"taka\",\"manat\",\"dram\",\"kwanza\",\"lek\",\"afghani\",\"renminbi\"]\n",
    "\n",
    "    \n",
    "    cand_right_tokens = list(get_right_ngrams(location,window=2))\n",
    "    #print len(cand_right_tokens)\n",
    "    #print cand_right_tokens#(get_right_ngrams(location,window=4))\n",
    "    for cand in cand_right_tokens:\n",
    "        #print \"[\"+cand+\"]\"\n",
    "        if cand not in list_currencies:\n",
    "            #print \"[\"+cand+\"]\"\n",
    "            #print location\n",
    "            return location\n",
    "    \n",
    "candidate_filter = location_currencies_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 82.5 ms, sys: 37.4 ms, total: 120 ms\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.candidates import CandidateExtractor\n",
    "\n",
    "candidate_extractor = CandidateExtractor(Location_Extraction,\n",
    "                                         [location_ngrams], [location_matcher],\n",
    "                                         candidate_filter=candidate_filter)\n",
    "\n",
    "\n",
    "#                         candidate_filter=candidate_filter\n",
    "\n",
    "%time candidate_extractor.apply(train_docs, split=0, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating for the test and dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 263\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 269\n",
      "CPU times: user 6.05 s, sys: 265 ms, total: 6.31 s\n",
      "Wall time: 7.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1)\n",
    "    print \"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create an .xlxs for the locations and document name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''create an .xlxs from location candidate\n",
    "    convert all words to lowercase and also change united states to usa'''\n",
    "\n",
    "import xlsxwriter\n",
    "all_cands = session.query(Location_Extraction).all()\n",
    "c_1 = all_cands[1]\n",
    "\n",
    "x=c_1.get_parent()\n",
    "y=str(x)\n",
    "\n",
    "def doc_name_extraction(inputstr):\n",
    "    p = inputstr.get_parent()\n",
    "    y = str(p)\n",
    "    return y.split(\"(\")[1].split(\",\")[0].split(\": \")[1]\n",
    "\n",
    "def location(data):\n",
    "    return data[0].get_span()\n",
    "\n",
    "# doc_name_extraction(y)\n",
    "dict_cands ={}\n",
    "\n",
    "for c in all_cands:\n",
    "    file_name = doc_name_extraction(c)\n",
    "    curr_loc = location(c)\n",
    "    if curr_loc ==\"United States\":\n",
    "        curr_loc = \"USA\"\n",
    "    if file_name in dict_cands.keys():\n",
    "        dict_cands[file_name].append(curr_loc.lower())\n",
    "        dict_cands[file_name] =list(set( dict_cands[file_name]))\n",
    "    else:\n",
    "        \n",
    "        dict_cands[file_name] = [curr_loc.lower()]\n",
    "\n",
    "workbook = xlsxwriter.Workbook('texas_locations_profiles_ver4.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "d = dict_cands\n",
    "row = 0\n",
    "col = 0\n",
    "worksheet.write(0, 0, \"file name\")\n",
    "worksheet.write(0, 1, \"location_pred\")\n",
    "for key in d.keys():\n",
    "    row += 1\n",
    "    worksheet.write(row, col, key)\n",
    "    \n",
    "    worksheet.write(row, col + 1, ','.join(map(str,list(set(d[key])))))\n",
    "workbook.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'760e158f-4de8-4311-869e-50ccb9f6b237'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cands[1].get_parent().document.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Repeating for development and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 263\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 269\n",
      "CPU times: user 9.28 s, sys: 542 ms, total: 9.82 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1)\n",
    "    print \"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dev_cands = session.query(Location_Extraction).filter(Location_Extraction.split == 1).all()\n",
    "# print \"Number of candidates:\", len(train_cands)\n",
    "# dev_cand1= dev_cands[300]\n",
    "# # for cand in dev_cand:\n",
    "# #     print cand\n",
    "# #     print cand.get_parent()\n",
    "# print get_ancestor_tag_names(dev_cand1)\n",
    "# print dev_cand1.get_parent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a markup file for the locations and document name, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787\n"
     ]
    }
   ],
   "source": [
    "'''create an .xlxs from location candidate\n",
    "    convert all words to lowercase and also change united states to usa'''\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "from snorkel.contrib.fonduer.lf_helpers import *\n",
    "import xlsxwriter\n",
    "import csv\n",
    "import itertools\n",
    "all_cands = session.query(Location_Extraction).all()\n",
    "c_1 = all_cands[1]\n",
    "print len(all_cands)\n",
    "x=c_1.get_parent()\n",
    "y=str(x)\n",
    "\n",
    "def doc_name_extraction(c):\n",
    "    \n",
    "    return c.get_parent().document.name\n",
    "#     p = inputstr.get_parent()\n",
    "#     y = str(p)\n",
    "#     return y.split(\"(\")[1].split(\",\")[0].split(\": \")[1]\n",
    "\n",
    "def extracted_candidate(c):\n",
    "#     return data[0].get_span()\n",
    "    return c.location.get_span().encode('ascii','ignore')\n",
    "def extracted_text_list(c):\n",
    "    return c.get_parent().text.encode('ascii','ignore')\n",
    "    #return c.get_parent().words.encode('ascii','ignore')\n",
    "# def candidate_right_token(c):\n",
    "#     x=list(get_right_tokens(c,window=40))\n",
    "#     lst=[]\n",
    "#     for word in x:\n",
    "#         lst.append(word.encode('ascii','ignore'))\n",
    "#     return lst\n",
    "# def candidate_left_token(c):\n",
    "#     x=list(get_left_tokens(c,window=40))\n",
    "    \n",
    "#     lst=[]\n",
    "#     for word in x:\n",
    "#         word=  word.encode('ascii','ignore')\n",
    "#         lst.append(word)\n",
    "#     return lst\n",
    "def candidate_neighbor_phrase(c):\n",
    "    x=list(get_neighbor_phrase_ngrams(c, d=4, attrib='words', n_min=1, n_max=1, lower=True))\n",
    "    lst=[]\n",
    "    for word in x:\n",
    "        lst.append(word.encode('ascii','ignore'))\n",
    "    return lst\n",
    "def candidate_xpath(c):\n",
    "    x =c[0].get_parent().xpath.encode('ascii','ignore')\n",
    "    #print x\n",
    "    \n",
    "    return x\n",
    "\n",
    "def candidate_strating_word(c):\n",
    "    return c[0].get_word_start()\n",
    "\n",
    "def candidate_ending_word(c):\n",
    "    return c[0].get_word_end()\n",
    "# doc_name_extraction(y)\n",
    "\n",
    "# def span_HTML_attribute (c):\n",
    "#     ll = get_attributes(c)\n",
    "#     t = type(ll)\n",
    "#     if t == list:\n",
    "#     lst=[]\n",
    "#     for l in ll:\n",
    "#         lst.append(l.encode('ascii','ignore'))\n",
    "#     return lst\n",
    "#     else:\n",
    "#         return ll.encode('ascii','ignore')\n",
    "def span_HTML_attribute (c):\n",
    "    ll = get_attributes(c)\n",
    "    t = type(ll)\n",
    "    if t == list:\n",
    "        lst=[]\n",
    "        for l in ll:\n",
    "            lst.append(l.encode('ascii','ignore'))\n",
    "        return lst\n",
    "    else:\n",
    "        return ll.encode('ascii','ignore')\n",
    "    \n",
    "result_list=list()\n",
    "counter = 0\n",
    "for c in all_cands:\n",
    "    \n",
    "    dict_cands ={}\n",
    "    file_name = doc_name_extraction(c)\n",
    "    cur_key = str(file_name)+\"_\"+ str(counter)\n",
    "    curr_loc = extracted_candidate(c)\n",
    "    \n",
    "  #  if curr_loc ==\"United States\":\n",
    "  #      curr_loc = \"USA\"\n",
    "#     if file_name in dict_cands.keys():\n",
    "#         dict_cands[file_name].append(curr_loc.lower())\n",
    "#         dict_cands[file_name] =list(set( dict_cands[file_name]))\n",
    "        #print \"hi\"\n",
    "    dict_cands[\"extracted_candidate\"] = [curr_loc.lower()]\n",
    "    \n",
    "  #  else:\n",
    "        \n",
    "    dict_cands[\"extracted_candidate\"] = [curr_loc.lower()]\n",
    "    dict_cands[\"file_name\"] = file_name\n",
    "    dict_cands[\"text_list\"] = extracted_text_list(c)\n",
    "    dict_cands[\"HTML attributes of span\"]= span_HTML_attribute (c)\n",
    "    dict_cands[\"candidate_neighbor_phrase\"] = candidate_neighbor_phrase(c)\n",
    "    dict_cands[\"started_word\"] = candidate_strating_word(c)\n",
    "    dict_cands[\"ended_word\"] = candidate_ending_word(c)\n",
    "    \n",
    "#     dict_cands[\"right_token\"]= candidate_right_token(c)\n",
    "#     dict_cands[\"left_token\"] = candidate_left_token(c)\n",
    "    dict_cands[\"candidate_xpath\"] = candidate_xpath(c)\n",
    "    dict_cands_2 = {}\n",
    "    dict_cands_2[cur_key] = dict_cands\n",
    "    result_list.append(dict_cands_2)\n",
    "    counter =counter+1\n",
    "    \n",
    "\n",
    "nested = result_list\n",
    "\n",
    "headings = [d.keys()[0] for d in nested]\n",
    "entries = [sorted(nested[index][col].items()) for index, col in enumerate(headings)]    \n",
    "#print headings\n",
    "#print \"hi\"\n",
    "with open('output_7.csv', 'wb') as f_output:\n",
    "    csv_output = csv.writer(f_output)\n",
    "    csv_output.writerow(['mykey'] + headings)\n",
    "\n",
    "    for cols in itertools.izip_longest(*entries, fillvalue=['<n/a>']*len(entries[0])):\n",
    "        csv_output.writerow([cols[0][0]] + [col[1] for col in cols])\n",
    "\n",
    "\n",
    "from itertools import izip\n",
    "from csv import reader, writer\n",
    "\n",
    "with open('output_7.csv') as f:\n",
    "    with open('destination_3.csv', 'w') as fw:\n",
    "        writer(fw, delimiter=',').writerows(izip(*reader(f, delimiter=',')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring more:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cand_16= train_cands[16]\n",
    "p =  cand_16.get_parent()\n",
    "\n",
    "print \"text for the 16th candidate:\\n\", cand_16.get_parent()\n",
    "#print common_ancestor(cand_16)\n",
    "#print \"16th candidate\\n:\",cand_16[0]\n",
    "ance_16 = get_ancestor_tag_names(cand_16)\n",
    "print \"ancestor of 16th candidate\\n:\", ance_16 \n",
    "#print ance_16.get_span()\n",
    "print \"***************************************************\"\n",
    "cand_17= train_cands[17]\n",
    "print \"text for the 17th candidate:\\n\", cand_17.get_parent()\n",
    "print \"17th candidate:\",cand_17\n",
    "ance_17 = get_ancestor_tag_names(cand_17)\n",
    "print \"ancestor of 17th candidate\\n:\", ance_17\n",
    "print \"***************************************************\"\n",
    "\n",
    "cand_19= train_cands[19]\n",
    "print \"text for the 19th candidate:\\n\", cand_19.get_parent()\n",
    "print \"19th candidate:\",cand_19\n",
    "print type(cand_19)\n",
    "ance_19 = get_ancestor_tag_names(cand_17)\n",
    "print \"ancestor of 19th candidate\\n:\", ance_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cand_16= train_cands[16]\n",
    "print cand_16\n",
    "\n",
    "cand_18= train_cands[18]\n",
    "print cand_18\n",
    "cand_19= train_cands[19]\n",
    "print cand_19\n",
    "print cand_19[0].get_span()\n",
    "get_phrase_ngrams(cand_19[0].get_span())\n",
    "train_doc[19]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
