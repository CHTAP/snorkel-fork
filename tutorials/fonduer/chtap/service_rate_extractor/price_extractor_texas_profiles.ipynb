{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_extractor_texas\n"
     ]
    }
   ],
   "source": [
    "## http://web.stanford.edu/~lwhsiao/api/\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PARALLEL = 4 # assuming a quad-core machine\n",
    "ATTRIBUTE = \"entity_phone\"\n",
    "#os.environ['SNORKELDB']\n",
    "os.environ['SNORKELDBNAME'] = \"price_extractor_texas\"\n",
    "print os.environ['SNORKELDBNAME']\n",
    "os.environ['SNORKELDB'] = 'postgresql://localhost:5432/' + os.environ['SNORKELDBNAME']\n",
    "\n",
    "\n",
    "sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Defining a Candidate Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "\n",
    "Published_Hourly_Price = candidate_subclass('published_hourly_price', [\"price\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and Transforming the Input Documents into Unified Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "\n",
    "docs_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/data/texas_profiles_data/'\n",
    "\n",
    "doc_preprocessor = HTMLPreprocessor(docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring an `OmniParser`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 10.3 s, sys: 327 ms, total: 10.6 s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = OmniParser(structural=True, lingual=True)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 342\n",
      "Phrases: 71150\n",
      "Table 832\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.models import Document, Phrase,Table\n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Phrases:\", session.query(Phrase).count()\n",
    "print \"Table\", session.query(Table).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the Corpus into Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 274\n",
      "dev: 34\n",
      "test: 34\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "splits = (0.8, 0.9)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "from pprint import pprint\n",
    "#pprint([x.name for x in train_docs])\n",
    "print \"train:\",len(train_docs)\n",
    "print \"dev:\" ,len(dev_docs)\n",
    "print \"test:\",len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.matchers import *\n",
    "import re\n",
    "\n",
    "number_matcher = NumberMatcher(longest_match_only=True) \n",
    "#price_regex = re.compile(r'[0-9]+[\\.\\,]?[0-9]*[:blank:]*\\$')\n",
    "#price_matcher_1= RegexMatchSpan(rgx =u'[0-9]+[\\.\\,]?[0-9]*[:blank:]*\\$', longest_match_only = True)\n",
    "#price_matcher_2= RegexMatchSpan(rgx =u'$\\[0-9]+[\\.\\,]?[0-9]*[:blank:]', longest_match_only = True)\n",
    "\n",
    "####Define a relation's ContextSpaces\n",
    "\n",
    "from snorkel.contrib.fonduer.fonduer.candidates import OmniNgrams\n",
    "price_matcher= RegexMatchSpan(rgx =u'(\\d+).(\\d+)', longest_match_only = True)\n",
    "\n",
    "##part_matcher = Union(price_matcher_1, price_matcher_2)\n",
    "price_ngrams = OmniNgrams(n_max=6, split_tokens=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining candidate Throttlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.lf_helpers import *\n",
    "from snorkel.contrib.fonduer.fonduer.lf_helpers import *\n",
    "def right_ngrams_price_filter(cand):\n",
    "    lst = ['/hour','hr','hour']\n",
    "\n",
    "    cand_right_negrams= list(get_right_ngrams(cand, window =3))\n",
    "   \n",
    "    if \"half\" not in cand_right_negrams:\n",
    "        for token in cand_right_negrams:\n",
    "            if token in lst:\n",
    "                return cand\n",
    "    else:\n",
    "        return False\n",
    "            \n",
    "        \n",
    "            \n",
    "candidate_filter=right_ngrams_price_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 89.7 ms, sys: 62.2 ms, total: 152 ms\n",
      "Wall time: 8.33 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.candidates import CandidateExtractor\n",
    "\n",
    "candidate_extractor = CandidateExtractor(Published_Hourly_Price,\n",
    "                                         [price_ngrams], [price_matcher],\n",
    "                                         candidate_filter=candidate_filter)\n",
    "\n",
    "\n",
    "#                         candidate_filter=candidate_filter\n",
    "\n",
    "%time candidate_extractor.apply(train_docs, split=0, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 186\n"
     ]
    }
   ],
   "source": [
    "train_cands = session.query(Published_Hourly_Price).filter(Published_Hourly_Price.split == 0).all()\n",
    "print \"Number of candidates:\", len(train_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 27\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 22\n",
      "CPU times: user 3.69 s, sys: 175 ms, total: 3.86 s\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1)\n",
    "    print \"Number of candidates:\", session.query(Published_Hourly_Price).filter(Published_Hourly_Price.split == i+1).count()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "text for the 9th candidate:\n",
      "Phrase (Doc: 57f9a1f2-9109-4038-bdf5-b6e49a9632fe, Index: 52, Text: $ 150.00   /half hour,       $ 250.00   /hour     ,   $ 500.00   /2 hours    ,   $ 1,000.00   /overnight)\n",
      "9th candidate\n",
      ": published_hourly_price(Span(\"250.00\", sentence=219837, chars=[31,36], words=[7,7]))\n",
      "text for the 10th candidate:\n",
      "Phrase (Doc: 8aa5b460-43f8-472c-81ed-efc6dd3485cf, Index: 44, Text: $ 80.00   /half hour,       $ 120.00   /hour)\n",
      "10th candidate\n",
      ": published_hourly_price(Span(\"120.00\", sentence=230844, chars=[30,35], words=[7,7]))\n",
      "#########################\n",
      "text for the 12th candidate:\n",
      "Phrase (Doc: 061b72cd-0d89-44e6-9a6f-7324d937286e, Index: 44, Text: $ 150.00   /half hour,       $ 200.00   /hour     ,   $ 300.00   /2 hours    ,   $ 600.00   /overnight)\n",
      "12th candidate\n",
      ": published_hourly_price(Span(\"200.00\", sentence=196737, chars=[31,36], words=[7,7]))\n",
      "########################################\n",
      "text for the 100th candidate:\n",
      "Phrase (Doc: aed7fee8-ede0-460c-bdf7-439ea797b707, Index: 50, Text: $ 120.00   /half hour,       $ 220.00   /hour)\n",
      "100th candidate\n",
      ": published_hourly_price(Span(\"220.00\", sentence=238488, chars=[31,36], words=[7,7]))\n",
      "###################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cands = session.query(Published_Hourly_Price).filter(Published_Hourly_Price.split==0).all()\n",
    "#session.query(Location_Extraction).filter(Location_Extraction.split == 0).all()\n",
    "print len(train_cands)\n",
    "cands9= train_cands[9]\n",
    "print \"text for the 9th candidate:\\n\", cands9.get_parent()\n",
    "print \"9th candidate\\n:\",cands9\n",
    "cands10= train_cands[10]\n",
    "print \"text for the 10th candidate:\\n\", cands10.get_parent()\n",
    "print \"10th candidate\\n:\",cands10\n",
    "print \"#########################\"\n",
    "cands12= train_cands[12]\n",
    "print \"text for the 12th candidate:\\n\", cands12.get_parent()\n",
    "print \"12th candidate\\n:\",cands12\n",
    "print \"########################################\"\n",
    "# ance_16 = get_ancestor_tag_names(cands10)\n",
    "# print \"ancestor of 16th candidate\\n:\", ance_16 \n",
    "cands100= train_cands[100]\n",
    "print \"text for the 100th candidate:\\n\", cands100.get_parent()\n",
    "print \"100th candidate\\n:\",cands100\n",
    "print \"###################################################\"\n",
    "\n",
    "all_cands = session.query(Published_Hourly_Price).all()\n",
    "len(all_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "published_hourly_price(Span(\"450.00\", sentence=226831, chars=[2,7], words=[1,1]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_ngrams_price_filter(cands200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create an .xlxs from price candidate\n",
    "    '''\n",
    "\n",
    "import xlsxwriter\n",
    "all_cands = session.query(Published_Hourly_Price).all()\n",
    "\n",
    "c_1 = all_cands[1]\n",
    "\n",
    "x=c_1.get_parent()\n",
    "y=str(x)\n",
    "\n",
    "def doc_name_extractor(inputstr):\n",
    "    return inputstr.get_parent().document.name\n",
    "#     p = inputstr.get_parent()\n",
    "#     y = str(p)\n",
    "#     return y.split(\"(\")[1].split(\",\")[0].split(\": \")[1]\n",
    "\n",
    "def price(data):\n",
    "    return data[0].get_span()\n",
    "\n",
    "# doc_name_extraction(y)\n",
    "dict_cands ={}\n",
    "\n",
    "for c in all_cands:\n",
    "    file_name = doc_name_extractor(c)\n",
    "    curr_price = price(c)\n",
    "    #if curr_loc ==\"United States\":\n",
    "        #curr_loc = \"USA\"\n",
    "    if file_name in dict_cands.keys():\n",
    "        dict_cands[file_name].append(curr_price.lower())\n",
    "        dict_cands[file_name] =list(set( dict_cands[file_name]))\n",
    "    else:\n",
    "        \n",
    "        dict_cands[file_name] = [curr_price.lower()]\n",
    "\n",
    "workbook = xlsxwriter.Workbook('texas_hourly_price_ver3.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "d = dict_cands\n",
    "row = 0\n",
    "col = 0\n",
    "worksheet.write(0, 0, \"file name\")\n",
    "worksheet.write(0, 1, \"price_pred\")\n",
    "for key in d.keys():\n",
    "    row += 1\n",
    "    worksheet.write(row, col, key)\n",
    "    \n",
    "    worksheet.write(row, col + 1, ','.join(map(str,list(set(d[key])))))\n",
    "workbook.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
